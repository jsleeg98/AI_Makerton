{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 결과 코드 실행 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 현재 위치 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/twopiece/src/yolov5'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. yolov5로 위치 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/twopiece/src/yolov5\n"
     ]
    }
   ],
   "source": [
    "%cd yolov5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. val.py 실행하여 예측 결과와 제출 txt 파일 만들기\n",
    "* yolov5/runs/val/labels에 txt 파일이 저장된다.\n",
    "* --data에 yaml 파일을 설정하면 된다.\n",
    "* 실행하면 yolov5/runs/val/에 result로 결과가 출력된 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mdata=/home/twopiece/datasets_rw/final-dataset/data.yaml, weights=yolov5x_trained_best.pt, batch_size=32, imgsz=416, conf_thres=0.001, iou_thres=0.6, task=val, device=, single_cls=False, augment=False, verbose=False, save_txt=True, save_hybrid=False, save_conf=True, save_json=False, project=runs/val, name=result , exist_ok=False, half=False\n",
      "YOLOv5 🚀 v5.0-507-g4cf7d48 torch 1.7.1+cu110 CUDA:0 (A100-SXM4-40GB MIG 2g.10gb, 9984.0MB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 476 layers, 87205423 parameters, 0 gradients, 217.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '/home/twopiece/datasets_rw/final-dataset/val.cache' images and la\u001b[0m\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        977       2972      0.995      0.987      0.995       0.86\n",
      "              helmat        977       1930      0.996      0.989      0.995      0.874\n",
      "                head        977       1042      0.993      0.985      0.995      0.846\n",
      "Speed: 0.1ms pre-process, 23.1ms inference, 1.5ms NMS per image at shape (32, 3, 416, 416)\n",
      "Results saved to \u001b[1mruns/val/result \u001b[0m\n",
      "977 labels saved to runs/val/result /labels\n"
     ]
    }
   ],
   "source": [
    "!python val.py --data /home/twopiece/datasets_rw/final-dataset/data.yaml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['yolov5x_trained_best.pt', 'yolov5l_trained_best.pt'], source=/home/twopiece/datasets_rw/detect-test-2, imgsz=[416, 416], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=True, save_conf=True, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=True, visualize=False, update=False, project=runs/detect, name=result, exist_ok=False, line_thickness=3, hide_labels=True, hide_conf=False, half=False\n",
      "YOLOv5 🚀 v5.0-507-g4cf7d48 torch 1.7.1+cu110 CUDA:0 (A100-SXM4-40GB MIG 2g.10gb, 9984.0MB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 476 layers, 87205423 parameters, 0 gradients, 217.1 GFLOPs\n",
      "Fusing layers... \n",
      "Model Summary: 367 layers, 46113663 parameters, 0 gradients, 107.8 GFLOPs\n",
      "Ensemble created with ['yolov5x_trained_best.pt', 'yolov5l_trained_best.pt']\n",
      "\n",
      "image 1/22 /home/twopiece/datasets_rw/detect-test-2/BikesHelmets254.png: 416x416 3 helmats, 1 head, Done. (0.134s)\n",
      "image 2/22 /home/twopiece/datasets_rw/detect-test-2/BikesHelmets383.png: 416x416 2 heads, Done. (0.130s)\n",
      "image 3/22 /home/twopiece/datasets_rw/detect-test-2/BikesHelmets41.png: 320x416 1 helmat, 2 heads, Done. (0.141s)\n",
      "image 4/22 /home/twopiece/datasets_rw/detect-test-2/BikesHelmets417.png: 288x416 1 head, Done. (0.143s)\n",
      "image 5/22 /home/twopiece/datasets_rw/detect-test-2/BikesHelmets439.png: 256x416 9 helmats, 8 heads, Done. (0.145s)\n",
      "image 6/22 /home/twopiece/datasets_rw/detect-test-2/BikesHelmets47.png: 288x416 4 heads, Done. (0.138s)\n",
      "image 7/22 /home/twopiece/datasets_rw/detect-test-2/BikesHelmets674.png: 352x416 2 helmats, 2 heads, Done. (0.133s)\n",
      "image 8/22 /home/twopiece/datasets_rw/detect-test-2/hard_hat_workers1535.png: 416x416 8 helmats, 2 heads, Done. (0.130s)\n",
      "image 9/22 /home/twopiece/datasets_rw/detect-test-2/hard_hat_workers1537.png: 416x416 1 helmat, 7 heads, Done. (0.132s)\n",
      "image 10/22 /home/twopiece/datasets_rw/detect-test-2/hard_hat_workers1542.png: 416x416 4 helmats, Done. (0.130s)\n",
      "image 11/22 /home/twopiece/datasets_rw/detect-test-2/hard_hat_workers1545.png: 416x416 2 helmats, Done. (0.131s)\n",
      "image 12/22 /home/twopiece/datasets_rw/detect-test-2/hard_hat_workers1562.png: 416x416 6 helmats, Done. (0.130s)\n",
      "image 13/22 /home/twopiece/datasets_rw/detect-test-2/hard_hat_workers1564.png: 416x416 5 helmats, 1 head, Done. (0.130s)\n",
      "image 14/22 /home/twopiece/datasets_rw/detect-test-2/hard_hat_workers1604.png: 416x416 8 helmats, 8 heads, Done. (0.131s)\n",
      "image 15/22 /home/twopiece/datasets_rw/detect-test-2/hard_hat_workers1688.png: 416x416 4 helmats, Done. (0.130s)\n",
      "image 16/22 /home/twopiece/datasets_rw/detect-test-2/hard_hat_workers1773.png: 416x416 5 heads, Done. (0.129s)\n",
      "image 17/22 /home/twopiece/datasets_rw/detect-test-2/hard_hat_workers1913.png: 416x416 1 helmat, Done. (0.132s)\n",
      "image 18/22 /home/twopiece/datasets_rw/detect-test-2/hard_hat_workers1919.png: 416x416 2 helmats, 5 heads, Done. (0.130s)\n",
      "image 19/22 /home/twopiece/datasets_rw/detect-test-2/hard_hat_workers2013.png: 416x416 4 helmats, Done. (0.129s)\n",
      "image 20/22 /home/twopiece/datasets_rw/detect-test-2/hard_hat_workers2071.png: 416x416 7 helmats, Done. (0.130s)\n",
      "image 21/22 /home/twopiece/datasets_rw/detect-test-2/hard_hat_workers2152.png: 416x416 20 helmats, Done. (0.131s)\n",
      "image 22/22 /home/twopiece/datasets_rw/detect-test-2/hard_hat_workers2326.png: 416x416 5 helmats, Done. (0.130s)\n",
      "Speed: 0.2ms pre-process, 132.7ms inference, 0.9ms NMS per image at shape (1, 3, 416, 416)\n",
      "Results saved to \u001b[1mruns/detect/result4\u001b[0m\n",
      "22 labels saved to runs/detect/result4/labels\n"
     ]
    }
   ],
   "source": [
    "!python detect.py --weights yolov5x_trained_best.pt yolov5l_trained_best.pt --img 416 --source  /home/twopiece/datasets_rw/detect-test-2 --hide-labels --augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
